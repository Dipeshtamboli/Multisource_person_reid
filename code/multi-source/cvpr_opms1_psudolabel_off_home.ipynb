{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sklearn as sk\n",
    "import time\n",
    "import random as rand\n",
    "from random import randrange\n",
    "#from numpy import array\n",
    "#from numpy.linalg import norm\n",
    "from flip_gradient import flip_gradient\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csgraph\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import *\n",
    "from sklearn.metrics import recall_score\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitarr(slist, k, index):\n",
    "    p=index[0]\n",
    "    st1=np.asarray(slist[p])\n",
    "    for i in range(k-1):\n",
    "        \n",
    "        q=index[i+1]\n",
    "        \n",
    "        st2=np.asarray(slist[q])\n",
    "        \n",
    "        ar=np.vstack((st1, st2))\n",
    "        \n",
    "        st1=ar\n",
    "        \n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aTolistC(array, class_l, n_classes):\n",
    "    \n",
    "    slist=[]\n",
    "    l_list=[]\n",
    "    \n",
    "    for j in range(n_classes):\n",
    "        k=[i for i,x in enumerate(class_l) if x == j]\n",
    "        \n",
    "        slist.append(array[k,:])\n",
    "        l_list.append(class_l[k])\n",
    "        \n",
    "    return slist, l_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGen_p(batch_size, source1, label1, source2, label2, source3, label3,source4, psudo, psudo_l):\n",
    "    \n",
    "    total_batch = 25\n",
    "    \n",
    "    src1=[]\n",
    "    lab1=[]\n",
    "    src2=[]\n",
    "    lab2=[]\n",
    "    src3=[]\n",
    "    lab3=[]\n",
    "    src4=[]\n",
    "    \n",
    "    \n",
    "    la_m2=tf.argmax(label2, 1)\n",
    "    la_m3=tf.argmax(label3, 1)\n",
    "    with tf.Session() as sess:\n",
    "        la_m2=sess.run(la_m2)\n",
    "        la_m3=sess.run(la_m3)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        uidx=[]\n",
    "        uidx1=[]\n",
    "        idx1 = rand.sample(range(1, source1.shape[0]), batch_size)\n",
    "        src1.append(source1[idx1,:])\n",
    "        lab1.append(label1[idx1,:])\n",
    "        lab=label1[idx1,:]\n",
    "        la_m=tf.argmax(label1[idx1,:], 1)\n",
    "        with tf.Session() as sess:\n",
    "            la_m=sess.run(la_m)\n",
    "        for j in range(batch_size):\n",
    "            p=la_m[j]\n",
    "            #print(p)\n",
    "            result = [index for index, word in enumerate(la_m2) if word == p]\n",
    "            q=randint(0, len(result)-1)\n",
    "            #print(result[q])\n",
    "            uidx.append(result[q])\n",
    "            \n",
    "        for jj in range(batch_size):\n",
    "            p=la_m[jj]\n",
    "            result1 = [index for index, word in enumerate(la_m3) if word == p]\n",
    "            q1=randint(0, len(result1)-1)\n",
    "            uidx1.append(result1[q1])\n",
    "            \n",
    "            \n",
    "            \n",
    "        src2.append(source2[uidx,:])\n",
    "        lab2.append(label2[uidx,:])\n",
    "        \n",
    "        src3.append(source3[uidx1,:])\n",
    "        lab3.append(label3[uidx1,:])\n",
    "        \n",
    "  \n",
    "        \n",
    "        idx3 = rand.sample(range(1, source4.shape[0]), batch_size)\n",
    "        src4.append(source4[idx3,:])\n",
    "        \n",
    "    \n",
    "    psudo_renge = int(psudo.shape[0]/batch_size)\n",
    "    \n",
    "    #print(\"kkkkkkkkkkkkkkkkkkkkkkkkkkk\")\n",
    "    #print(psudo)\n",
    "    #print(\"hhhhhhhhhhhhhhhhhhhhhhhhhhhh\")\n",
    "    #print(psudo[0])\n",
    "    #print(\"lllllllllllllllllllllllll\")\n",
    "    \n",
    "    #print(psudo[0:5])\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(psudo_renge):\n",
    "        psu = psudo[start:start+batch_size]\n",
    "        psu_l = psudo_l[start:start+batch_size]\n",
    "        \n",
    "        src1.append(psu)\n",
    "        lab1.append(psu_l)\n",
    "        \n",
    "        src2.append(psu)\n",
    "        \n",
    "        lab2.append(psu_l)\n",
    "        \n",
    "        src3.append(psu)\n",
    "        \n",
    "        lab3.append(psu_l)\n",
    "        \n",
    "        src4.append(psu)\n",
    "        \n",
    "        start = start+batch_size\n",
    "    total_batch_after_p = len(src1)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "       \n",
    "    return total_batch_after_p, src1,lab1,src2,lab2, src3, lab3, src4\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGen(batch_size, source1, label1, source2, label2, source3, label3, source4):\n",
    "    \n",
    "    total_batch = 25\n",
    "    \n",
    "    src1=[]\n",
    "    lab1=[]\n",
    "    src2=[]\n",
    "    lab2=[]\n",
    "    src3=[]\n",
    "    lab3=[]\n",
    "    lab3=[]\n",
    "    src4=[]\n",
    "    \n",
    "    \n",
    "    la_m2=tf.argmax(label2, 1)\n",
    "    la_m3=tf.argmax(label3, 1)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        la_m2=sess.run(la_m2)\n",
    "        la_m3=sess.run(la_m3)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        uidx=[]\n",
    "        uidx1=[]\n",
    "        idx1 = rand.sample(range(1, source1.shape[0]), batch_size)\n",
    "        src1.append(source1[idx1,:])\n",
    "        lab1.append(label1[idx1,:])\n",
    "        lab=label1[idx1,:]\n",
    "        la_m=tf.argmax(label1[idx1,:], 1)\n",
    "        with tf.Session() as sess:\n",
    "            la_m=sess.run(la_m)\n",
    "        for j in range(batch_size):\n",
    "            p=la_m[j]\n",
    "            #print(p)\n",
    "            result = [index for index, word in enumerate(la_m2) if word == p]\n",
    "            q=randint(0, len(result)-1)\n",
    "            #print(result[q])\n",
    "            uidx.append(result[q])\n",
    "            \n",
    "        for jj in range(batch_size):\n",
    "            p=la_m[jj]\n",
    "            result1 = [index for index, word in enumerate(la_m3) if word == p]\n",
    "            q1=randint(0, len(result1)-1)\n",
    "            uidx1.append(result1[q1])\n",
    "            \n",
    "            \n",
    "        src2.append(source2[uidx,:])\n",
    "        lab2.append(label2[uidx,:])\n",
    "        \n",
    "        src3.append(source3[uidx1,:])\n",
    "        lab3.append(label3[uidx1,:])\n",
    "        \n",
    "        \n",
    "    \n",
    "        idx4 = rand.sample(range(1, source4.shape[0]), batch_size)\n",
    "        src4.append(source4[idx4,:])\n",
    "       \n",
    "    return src1,lab1,src2,lab2, src3, lab3, src4\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file name \n",
    "art_ = sio.loadmat('OffHoRes/art/a_f.mat')\n",
    "art_l_ = sio.loadmat('OffHoRes/art/a_l.mat')\n",
    "\n",
    "clipart_ = sio.loadmat('OffHoRes/clipart/c_f.mat')\n",
    "clipart_l_ = sio.loadmat('OffHoRes/clipart/c_l.mat')\n",
    "\n",
    "product_ = sio.loadmat('OffHoRes/product/p_f.mat')\n",
    "product_l_ = sio.loadmat('OffHoRes/product/p_l.mat')\n",
    "\n",
    "real_world_ = sio.loadmat('OffHoRes/real_world/r_f.mat')\n",
    "real_world_l_ = sio.loadmat('OffHoRes/real_world/r_l.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = art_['a_f']\n",
    "art_l = art_l_['a_l']\n",
    "#art_l=art_l.ravel()\n",
    "\n",
    "clipart = clipart_['c_f']\n",
    "clipart_l = clipart_l_['c_l']\n",
    "#clipart_l = clipart_l.ravel()\n",
    "\n",
    "product = product_['p_f']\n",
    "product_l = product_l_['p_l']\n",
    "#product_l = product_l.ravel()\n",
    "\n",
    "real_world = real_world_['r_f']\n",
    "real_world_l = real_world_l_['r_l']\n",
    "#real_world_l = real_world_l.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 0],\n",
       "       [ 0],\n",
       "       ...,\n",
       "       [64],\n",
       "       [64],\n",
       "       [64]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "n_class = len(np.unique(art_l))\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "art=art.astype(np.float32)\n",
    "art_l=art_l.astype(np.float32)\n",
    "\n",
    "clipart=clipart.astype(np.float32)\n",
    "clipart_l=clipart_l.astype(np.float32)\n",
    "\n",
    "product=product.astype(np.float32)\n",
    "product_l=product_l.astype(np.float32)\n",
    "\n",
    "real_world=real_world.astype(np.float32)\n",
    "real_world_l=real_world_l.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "artS, artL=aTolistC(art, art_l, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipartS, clipartL=aTolistC(clipart, clipart_l, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "productS, productL=aTolistC(product, product_l, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_worldS, real_worldL=aTolistC(real_world, real_world_l, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_l=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,\n",
    "      29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]\n",
    "s2_l=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,\n",
    "      29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]\n",
    "\n",
    "s3_l=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,\n",
    "      29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s11=  artS\n",
    "s11_l=  artL\n",
    "\n",
    "s22=  productS\n",
    "s22_l= productL\n",
    "\n",
    "s33=  clipartS\n",
    "s33_l= clipartL\n",
    "\n",
    "tar = real_world\n",
    "tar_ll = real_world_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (source1)\n",
    "sou1= splitarr(s11, 45, s1_l)\n",
    "sou1_l=splitarr(s11_l, 45, s1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1789, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sou1_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (source2)\n",
    "sou2= splitarr(s22, 45, s2_l)\n",
    "sou2_l=splitarr(s22_l, 45, s2_l)\n",
    "#print(sou2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (source3)\n",
    "sou3= splitarr(s33, 45, s2_l)\n",
    "sou3_l=splitarr(s33_l, 45, s2_l)\n",
    "#print(sou2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou2_l = sou2_l.ravel()\n",
    "sou1_l = sou1_l.ravel()\n",
    "sou3_l = sou3_l.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh=np.unique(sou1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_llk = np.copy(tar_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       ...,\n",
       "       [64.],\n",
       "       [64.],\n",
       "       [64.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Label update\n",
    "r=tar_ll.shape[0]\n",
    "for i in range (r):\n",
    "    \n",
    "    if tar_ll[i] >= 45:\n",
    "        \n",
    "        tar_llk[i] = 45\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(tar_llk))\n",
    "print(n_classes)\n",
    "tar_llk = tar_llk.ravel()\n",
    "#web_llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_l = tar_llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou1_l_h=tf.one_hot(sou1_l, n_classes)\n",
    "sou2_l_h=tf.one_hot(sou2_l, n_classes)\n",
    "sou3_l_h=tf.one_hot(sou3_l, n_classes)\n",
    "tar_l_h=tf.one_hot(tar_l, n_classes)\n",
    "with tf.Session() as sess:\n",
    "    sou1_l_h=sess.run(sou1_l_h)\n",
    "    sou2_l_h=sess.run(sou2_l_h)\n",
    "    sou3_l_h=sess.run(sou3_l_h)\n",
    "    tar_l_h=sess.run(tar_l_h)\n",
    "#print(tar_l_h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b,c,d,e,g = BatchGen(10, sou1,sou1_l_h, sou2, sou2_l_h, web, tar_l_h )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \n",
    "    \n",
    "    initial = tf.random_normal(shape, stddev=0.1)\n",
    "    #initial = tf.truncated_normal(shape)#, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bise_variable(shape):\n",
    "   \n",
    "    initial = tf.random_normal(shape)\n",
    "    #initial = tf.constant(0.1, \"float32\", shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=sou1.shape[1]\n",
    "\n",
    "X1= tf.placeholder(tf.float32, [None, n_features], name='X1' ) # Source1 Input data \n",
    "Y1_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y1_ind')  # Source1 label index \n",
    "\n",
    "X2= tf.placeholder(tf.float32, [None, n_features], name='X2' ) # Source2 Input data \n",
    "Y2_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y2_ind')  # Source2 label index \n",
    "\n",
    "X3= tf.placeholder(tf.float32, [None, n_features], name='X3' ) # source3 Input data \n",
    "Y3_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y2_ind')  # Source3 label index \n",
    "\n",
    "X4= tf.placeholder(tf.float32, [None, n_features], name='X4' ) # Target Input data \n",
    "Y4_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y4_ind')  # target label index \n",
    "\n",
    "\n",
    "l = tf.placeholder(tf.float32, [], name= 'l')  # gradient reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=n_features    \n",
    "num_hid1=1200\n",
    "num_hid2=600\n",
    "num_hid3=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_1=tf.Variable(tf.random_normal([num_inputs, num_hid1], stddev = 0.01), name= 'w1_1')\n",
    "b1_1=tf.Variable(tf.random_normal([num_hid1]), name = 'b1_1')\n",
    "\n",
    "w1_2=tf.Variable(tf.random_normal([num_inputs, num_hid1], stddev = 0.01), name= 'w1_2')\n",
    "b1_2=tf.Variable(tf.random_normal([num_hid1]), name = 'b1_2')\n",
    "\n",
    "w2_1=tf.Variable(tf.random_normal([num_hid1, num_hid2], stddev = 0.01), name= 'w2_1')\n",
    "b2_1=tf.Variable(tf.random_normal([num_hid2]), name = 'b2_1')\n",
    "\n",
    "w2_2=tf.Variable(tf.random_normal([num_hid1, num_hid2]), name= 'w2_2')\n",
    "b2_2=tf.Variable(tf.random_normal([num_hid2]), name = 'b2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5_1=tf.Variable(tf.random_normal([num_hid2, num_hid3], stddev = 0.01), name= 'w5_1')\n",
    "b5_1=tf.Variable(tf.random_normal([num_hid3]), name = 'b5_1')\n",
    "\n",
    "w6_1=tf.Variable(tf.random_normal([num_hid3, n_classes], stddev = 0.01), name= 'w6_1')\n",
    "b6_1=tf.Variable(tf.random_normal([n_classes]), name = 'b6_1')\n",
    "\n",
    "\n",
    "\n",
    "w5_2=tf.Variable(tf.random_normal([num_hid2, num_hid3], stddev = 0.01), name= 'w5_2')\n",
    "b5_2=tf.Variable(tf.random_normal([num_hid3]), name = 'b5_2')\n",
    "\n",
    "w6_2=tf.Variable(tf.random_normal([num_hid3, n_classes], stddev = 0.01), name= 'w6_2')\n",
    "b6_2=tf.Variable(tf.random_normal([n_classes]), name = 'b6_2')\n",
    "\n",
    "w5_3=tf.Variable(tf.random_normal([num_hid2, num_hid3], stddev = 0.01), name= 'w5_3')\n",
    "b5_3=tf.Variable(tf.random_normal([num_hid3]), name = 'b5_3')\n",
    "\n",
    "w6_3=tf.Variable(tf.random_normal([num_hid3, n_classes], stddev = 0.01), name= 'w6_3')\n",
    "b6_3=tf.Variable(tf.random_normal([n_classes]), name = 'b6_3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z1, z2, z3, z4):\n",
    "    \n",
    "    #with tf.variable_scope(\"FeatureGenerator\",reuse=True):\n",
    "        \n",
    "    sh1_1=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(z1, w1_1) + b1_1))   # source1\n",
    "    #sh1_1 = Dropout(0.3)(sh1_1, training=True)\n",
    "    sh2_1=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(z2, w1_1) + b1_1))   # source2\n",
    "    #sh2_1 = Dropout(0.3)(sh2_1, training=True)\n",
    "    \n",
    "    sh3_1=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(z3, w1_1) + b1_1))   # source3\n",
    "    \n",
    "    T1=tf.nn.relu(tf.matmul(z4, w1_1) + b1_1)    # Target\n",
    "        \n",
    "    sh1_2=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(sh1_1, w2_1) + b2_1))   # source1\n",
    "    #sh1_2 = Dropout(0.3)(sh1_2, training=True)\n",
    "    sh2_2=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(sh2_1, w2_1) + b2_1))   # source2\n",
    "    #sh2_2 = Dropout(0.3)(sh2_2, training=True)\n",
    "    \n",
    "    sh3_2=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(sh3_1, w2_1) + b2_1))   # source3\n",
    "    #sh2_2 = Dropout(0.3)(sh2_2, training=True)\n",
    "    \n",
    "    T2=tf.nn.relu(tf.matmul(T1, w2_1) + b2_1)    # Target\n",
    "        \n",
    "    return sh1_2, sh2_2, sh3_2, T2\n",
    "\n",
    "    \n",
    "    \n",
    "def classifier(x1, x2, x3, t):\n",
    "    \n",
    "    #with tf.variable_scope(\"ClassClassifier\", reuse = True ):\n",
    "        \n",
    "    X11 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(x1, w5_1) + b5_1))   # 1st layer for source1\n",
    "    t11 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(t, w5_1) + b5_1))   # 1st layer target through source1\n",
    "    \n",
    "    p_logit_X1 = tf.matmul(X11, w6_1) + b6_1   # logit for source1\n",
    "    p_logit_t1 = tf.matmul(t11, w6_1) + b6_1   # logit for target through source1\n",
    "       \n",
    "        \n",
    "    X22 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(x2, w5_2) + b5_2))     # 2nd layer for source2\n",
    "    t22 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(t, w5_2) + b5_2))   # 2nd layer target through source2\n",
    "    \n",
    "    p_logit_X2 = tf.matmul(X22, w6_2) + b6_2   # logit for source2\n",
    "    p_logit_t2 = tf.matmul(t22, w6_2) + b6_2   # logit for target through source2\n",
    "        \n",
    "        \n",
    "    X33 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(x3, w5_3) + b5_3))     # 2nd layer for source3\n",
    "    t33 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(t, w5_3) + b5_3))   # 2nd layer target through source3\n",
    "    \n",
    "    p_logit_X3 = tf.matmul(X33, w6_3) + b6_3   # logit for source3\n",
    "    p_logit_t3 = tf.matmul(t33, w6_3) + b6_3   # logit for target through source3\n",
    "        \n",
    "    \n",
    "    p_logit_t = (p_logit_t1 + p_logit_t2 + p_logit_t3)/3\n",
    "        \n",
    "    p_X1 = tf.nn.softmax(p_logit_X1)\n",
    "        \n",
    "    p_X2 = tf.nn.softmax(p_logit_X2)\n",
    "    \n",
    "    p_X3 = tf.nn.softmax(p_logit_X3)\n",
    "        \n",
    "    p_t = tf.nn.softmax(p_logit_t)\n",
    "    \n",
    "    p_t1 = tf.argmax(p_t)\n",
    "        \n",
    "        \n",
    "    return p_logit_X1, p_logit_X2, p_logit_X3, p_logit_t, p_X1, p_X2, p_X3, p_t, p_t1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_f, s2_f, s3_f, t_f = generator(X1, X2, X3, X4)\n",
    "\n",
    "logit_x1, logit_x2, logit_x3, logit_t, px1, px2, px3, p_t, p_t1 = classifier(s1_f, s2_f, s3_f, t_f)\n",
    "\n",
    "t = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_loss_X1 = tf.nn.softmax_cross_entropy_with_logits(logits = logit_x1, labels = Y1_ind)\n",
    "p_loss_X2 = tf.nn.softmax_cross_entropy_with_logits(logits = logit_x2, labels = Y2_ind)\n",
    "p_loss_X3 = tf.nn.softmax_cross_entropy_with_logits(logits = logit_x3, labels = Y3_ind)\n",
    "\n",
    "p_loss_s1_s2 = tf.reduce_mean(tf.pow(s1_f - s2_f ,2), 1)\n",
    "p_loss_s3_s2 = tf.reduce_mean(tf.pow(s3_f - s2_f ,2), 1)\n",
    "p_loss_s1_s3 = tf.reduce_mean(tf.pow(s1_f - s3_f ,2), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_cl = tf.reduce_mean(p_loss_X1) + tf.reduce_mean(p_loss_X2) +tf.reduce_mean(p_loss_X3)\n",
    "\n",
    "loss1 = loss_cl + p_loss_s1_s2 + p_loss_s3_s2 +p_loss_s1_s3\n",
    "\n",
    "p_kone=tf.gather(p_t,indices=[n_classes-1],axis=1)\n",
    "Ladv=-0.5*tf.reduce_mean(tf.log(p_kone+1e-8))-0.5*tf.reduce_mean((tf.log(1.0-(p_kone)+1e-8)))\n",
    "\n",
    "jh= p_t[:, :n_classes-1]\n",
    "\n",
    "kh=tf.argmax(jh, axis = 1)\n",
    "pu_l_h=tf.one_hot(kh, n_classes)\n",
    "\n",
    "gbp=p_t[:, -1]\n",
    "hg = tf.reduce_mean(jh, 1)\n",
    "alpha = 0.2\n",
    "hing=tf.abs(p_t[:, -1] - hg) - alpha\n",
    "\n",
    "row = jh.shape[0]\n",
    "gg=0.0\n",
    "\n",
    "bb=[]\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    hgbw = tf.cond(hing[i]<gg, lambda: hing[i], lambda:gg)\n",
    "    \n",
    "    bb.append(hgbw)\n",
    "\n",
    "hingh_loss=tf.reduce_mean(tf.stack(bb))  # hingh loss\n",
    "\n",
    "loss2 = loss_cl + Ladv   # classifier\n",
    "\n",
    "loss3 = loss1 - Ladv - hingh_loss   # Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_v = [w1_1, w2_1, b1_1, b2_1]\n",
    "C_v = [w5_1, w5_2, w5_3, b5_1, b5_2, b5_3, w6_1, w6_2, w6_3, b6_1, b6_2,b6_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_cl_op = tf.train.AdamOptimizer(0.001).minimize(loss1)\n",
    "\n",
    "loss1C_op = tf.train.AdamOptimizer(0.001).minimize(loss2, var_list = C_v)\n",
    "\n",
    "loss2G_op = tf.train.AdamOptimizer(0.001).minimize(loss3, var_list = G_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y1_ind, 1), tf.argmax(px1, 1)), tf.float32))\n",
    "\n",
    "p2_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y2_ind, 1), tf.argmax(px2, 1)), tf.float32))\n",
    "\n",
    "p3_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y3_ind, 1), tf.argmax(px3, 1)), tf.float32))\n",
    "\n",
    "pt_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y4_ind, 1), tf.argmax(p_t, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('epoch#####555555555555##########################', 0)\n",
      "(46, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4357, 46]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-d465b8b6b9d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mOS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mOS_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1357\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4357, 46]"
     ]
    }
   ],
   "source": [
    "num_epoch=100\n",
    "batch_size=20\n",
    "dp1 = 0.0\n",
    "dp2 = 0.0\n",
    "dp3=0.0\n",
    "dp11 = 0.0\n",
    "dp12 = 0.0\n",
    "dp13=0.0\n",
    "\n",
    "#ps_t = []\n",
    "#ps_t_l = []\n",
    "\n",
    "flag = 0\n",
    "\n",
    "l1=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]\n",
    "l2=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        ps_t = []\n",
    "        ps_t_l = []\n",
    "        \n",
    "        if flag ==0:\n",
    "            total_batches = 25\n",
    "            \n",
    "            X1_batch,y1_batch, X2_batch,y2_batch, X3_batch, y3_batch, X4_batch=BatchGen(batch_size, sou1, sou1_l_h,\n",
    "                                                                    sou2, sou2_l_h, sou3, sou3_l_h, tar)\n",
    "        else:\n",
    "            total_batches, X1_batch,y1_batch, X2_batch,y2_batch, X3_batch, y3_batch, X4_batch=BatchGen_p(batch_size, sou1,\n",
    "                                                                                     sou1_l_h, sou2,sou2_l_h,\n",
    "                                                                                     sou3, sou3_l_h,\n",
    "                                                                                     tar, psudo, psudo_l)\n",
    "        \n",
    "        flag = 0\n",
    "\n",
    "        for batch in range(total_batches):\n",
    "            \n",
    "            xx_1=X1_batch[batch]\n",
    "            #print(xx_1.shape)\n",
    "            \n",
    "            xx_2=X2_batch[batch]\n",
    "            #print(xx_2.shape)\n",
    "            \n",
    "            xx_3=X3_batch[batch]\n",
    "            #print(xx_3.shape)\n",
    "            \n",
    "            xx_4=X4_batch[batch]\n",
    "            \n",
    "            yy_1=y1_batch[batch]\n",
    "            yy_2=y2_batch[batch]\n",
    "            yy_3=y3_batch[batch]\n",
    "                        \n",
    "\n",
    "            \n",
    "            #_,batch_loss_cl  = sess.run([loss_cl_op, loss1],\n",
    "                                        #feed_dict={X1: xx_1, X2:xx_2, Y1_ind:yy_1, Y2_ind:yy_2, X3:xx_3})\n",
    "            \n",
    "            _,batch_loss1C, pu_one_hot, pu_logit, pu_argmax  = sess.run([loss1C_op, loss2, pu_l_h, jh, kh],\n",
    "                                        feed_dict={X1: xx_1, X2:xx_2, Y1_ind:yy_1, Y2_ind:yy_2,\n",
    "                                                   X3:xx_3, Y3_ind:yy_3, X4:xx_4})\n",
    "            \n",
    "            \n",
    "            rr=0\n",
    "            for v in range(pu_one_hot.shape[0]):\n",
    "                \n",
    "                ff = pu_one_hot[v]\n",
    "                aa=pu_argmax[v]\n",
    "                dwq = tf.cast(ff, tf.int32)\n",
    "               \n",
    "                if pu_logit[rr, aa] >= 0.9:\n",
    "                    flag = 1\n",
    "                        \n",
    "                    ps_t.append(xx_4[v, :])\n",
    "                    ps_t_l.append(ff)\n",
    "                    \n",
    "    \n",
    "                rr=rr+1\n",
    "                    \n",
    "            \n",
    "            \n",
    "            _,batch_loss2G  = sess.run([loss2G_op, loss3],\n",
    "                                        feed_dict={X1: xx_1, X2:xx_2, Y1_ind:yy_1, Y2_ind:yy_2, X3:xx_3,\n",
    "                                                   Y3_ind:yy_3, X4:xx_4})\n",
    "            \n",
    "        #ps_t.append(xx_1)\n",
    "        #ps_t_l.append(yy_1)\n",
    "        psudo=np.asarray(ps_t)\n",
    "        psudo_l = np.asarray(ps_t_l)\n",
    "        \n",
    "        \n",
    "        print(len(ps_t))\n",
    "        p = epoch%1\n",
    "        \n",
    "        if p==0:\n",
    "            print(\"epoch#####555555555555##########################\", epoch)\n",
    "                    \n",
    "            pas11 = sess.run([p1_acc], feed_dict={X1:tar, Y1_ind:tar_l_h})\n",
    "            pas12 = sess.run([p2_acc], feed_dict={X2:tar, Y2_ind:tar_l_h})\n",
    "            pas13 = sess.run([p3_acc], feed_dict={X3:tar, Y3_ind:tar_l_h})\n",
    "            lab = sess.run([p_t1], feed_dict={X4:tar, Y4_ind:tar_l_h})\n",
    "            pass_avg2 = sess.run([pt_acc], feed_dict={X4:tar, Y4_ind:tar_l_h})\n",
    "            \n",
    "            lab = np.matrix.transpose(np.asarray(lab))\n",
    "            #print(tar_l.shape)\n",
    "            print(lab.shape)\n",
    "\n",
    "            OS = recall_score(tar_l, lab, labels=l2, average='macro')\n",
    "            OS_star = recall_score(tar_l, lab, labels=l1, average='macro')\n",
    "\n",
    "            print('target from source1', pas11)\n",
    "            print('target from source2', pas12)\n",
    "            print('target from source3', pas13)\n",
    "            print('OS', OS)\n",
    "            print('OS-star', OS_star)\n",
    "            print('OS-all', pass_avg2)\n",
    "        \n",
    "        if OS >= dp11:\n",
    "            \n",
    "            dp11 = OS\n",
    "            dp12 = OS_star\n",
    "            dp13 = pass_avg2\n",
    "            \n",
    "            dp1 = pas11\n",
    "            dp2 = pass12\n",
    "            dp3 = pass13\n",
    "            \n",
    "      \n",
    "        print('OS: %f OS-star: %f Avg: %f' %(dp11, dp12, dp13))\n",
    "\n",
    "                            \n",
    "                            \n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "    sess.close()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

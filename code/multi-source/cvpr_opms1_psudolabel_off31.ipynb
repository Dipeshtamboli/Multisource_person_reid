{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sklearn as sk\n",
    "import time\n",
    "import random as rand\n",
    "from random import randrange\n",
    "#from numpy import array\n",
    "#from numpy.linalg import norm\n",
    "from flip_gradient import flip_gradient\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csgraph\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import *\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitarr(slist, k, index):\n",
    "    p=index[0]\n",
    "    st1=np.asarray(slist[p])\n",
    "    for i in range(k-1):\n",
    "        \n",
    "        q=index[i+1]\n",
    "        \n",
    "        st2=np.asarray(slist[q])\n",
    "        \n",
    "        ar=np.vstack((st1, st2))\n",
    "        \n",
    "        st1=ar\n",
    "        \n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aTolistC(array, class_l, n_classes):\n",
    "    \n",
    "    slist=[]\n",
    "    l_list=[]\n",
    "    \n",
    "    for j in range(n_classes):\n",
    "        k=[i for i,x in enumerate(class_l) if x == j]\n",
    "        \n",
    "        slist.append(array[k,:])\n",
    "        l_list.append(class_l[k])\n",
    "        \n",
    "    return slist, l_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGen_p(batch_size, source1, label1, source2, label2, source3, psudo, psudo_l):\n",
    "    \n",
    "    total_batch = 50\n",
    "    \n",
    "    src1=[]\n",
    "    lab1=[]\n",
    "    src2=[]\n",
    "    lab2=[]\n",
    "    src3=[]\n",
    "    lab3=[]\n",
    "    \n",
    "    \n",
    "    la_m2=tf.argmax(label2, 1)\n",
    "    with tf.Session() as sess:\n",
    "        la_m2=sess.run(la_m2)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        uidx=[]\n",
    "        idx1 = rand.sample(range(1, source1.shape[0]), batch_size)\n",
    "        src1.append(source1[idx1,:])\n",
    "        lab1.append(label1[idx1,:])\n",
    "        lab=label1[idx1,:]\n",
    "        la_m=tf.argmax(label1[idx1,:], 1)\n",
    "        with tf.Session() as sess:\n",
    "            la_m=sess.run(la_m)\n",
    "        for j in range(batch_size):\n",
    "            p=la_m[j]\n",
    "            #print(p)\n",
    "            result = [index for index, word in enumerate(la_m2) if word == p]\n",
    "            q=randint(0, len(result)-1)\n",
    "            #print(result[q])\n",
    "            uidx.append(result[q])\n",
    "            \n",
    "        src2.append(source2[uidx,:])\n",
    "        lab2.append(label2[uidx,:])\n",
    "        \n",
    "  \n",
    "        \n",
    "        idx3 = rand.sample(range(1, source3.shape[0]), batch_size)\n",
    "        src3.append(source3[idx3,:])\n",
    "        \n",
    "    \n",
    "    psudo_renge = int(psudo.shape[0]/batch_size)\n",
    "    \n",
    "    #print(\"kkkkkkkkkkkkkkkkkkkkkkkkkkk\")\n",
    "    #print(psudo)\n",
    "    #print(\"hhhhhhhhhhhhhhhhhhhhhhhhhhhh\")\n",
    "    #print(psudo[0])\n",
    "    #print(\"lllllllllllllllllllllllll\")\n",
    "    \n",
    "    #print(psudo[0:5])\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(psudo_renge):\n",
    "        psu = psudo[start:start+batch_size]\n",
    "        psu_l = psudo_l[start:start+batch_size]\n",
    "        \n",
    "        src1.append(psu)\n",
    "        lab1.append(psu_l)\n",
    "        \n",
    "        src2.append(psu)\n",
    "        \n",
    "        lab2.append(psu_l)\n",
    "        \n",
    "        src3.append(psu)\n",
    "        \n",
    "        start = start+batch_size\n",
    "    total_batch_after_p = len(src1)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "       \n",
    "    return total_batch_after_p, src1,lab1,src2,lab2, src3\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGen(batch_size, source1, label1, source2, label2, source3):\n",
    "    \n",
    "    total_batch = 50\n",
    "    \n",
    "    src1=[]\n",
    "    lab1=[]\n",
    "    src2=[]\n",
    "    lab2=[]\n",
    "    src3=[]\n",
    "    lab3=[]\n",
    "    \n",
    "    \n",
    "    la_m2=tf.argmax(label2, 1)\n",
    "    with tf.Session() as sess:\n",
    "        la_m2=sess.run(la_m2)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        uidx=[]\n",
    "        idx1 = rand.sample(range(1, source1.shape[0]), batch_size)\n",
    "        src1.append(source1[idx1,:])\n",
    "        lab1.append(label1[idx1,:])\n",
    "        lab=label1[idx1,:]\n",
    "        la_m=tf.argmax(label1[idx1,:], 1)\n",
    "        with tf.Session() as sess:\n",
    "            la_m=sess.run(la_m)\n",
    "        for j in range(batch_size):\n",
    "            p=la_m[j]\n",
    "            #print(p)\n",
    "            result = [index for index, word in enumerate(la_m2) if word == p]\n",
    "            q=randint(0, len(result)-1)\n",
    "            #print(result[q])\n",
    "            uidx.append(result[q])\n",
    "            \n",
    "        src2.append(source2[uidx,:])\n",
    "        lab2.append(label2[uidx,:])\n",
    "    \n",
    "        idx3 = rand.sample(range(1, source3.shape[0]), batch_size)\n",
    "        src3.append(source3[idx3,:])\n",
    "       \n",
    "    return src1,lab1,src2,lab2, src3\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing csv module \n",
    "import csv \n",
    "\n",
    "\n",
    "def csvToarr(filename):\n",
    "    \n",
    "    # initializing the titles and rows list \n",
    "    fields = [] \n",
    "    rows = [] \n",
    "  \n",
    "    # reading csv file \n",
    "    with open(filename, 'r') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "      \n",
    "        # extracting field names through first row \n",
    "        fields = next(csvreader) \n",
    "  \n",
    "        # extracting each data row one by one \n",
    "        for row in csvreader: \n",
    "            rows.append(row) \n",
    "  \n",
    "        # get total number of rows \n",
    "    print(\"Total no. of rows: %d\"%(csvreader.line_num))\n",
    "        \n",
    "    return np.asarray(rows)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file name \n",
    "amazon = 'amazon_amazon.csv'\n",
    "digitalSlr = 'dslr_dslr.csv'\n",
    "webcam = 'webcam_webcam.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 2817\n",
      "Total no. of rows: 498\n",
      "Total no. of rows: 795\n"
     ]
    }
   ],
   "source": [
    "ama_=csvToarr(amazon)\n",
    "dslr_=csvToarr(digitalSlr)\n",
    "web_=csvToarr(webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ama_l=ama_[:,-1]\n",
    "ama=ama_[:,0:ama_.shape[1] -1]\n",
    "\n",
    "dslr_l=dslr_[:,-1]\n",
    "dslr=dslr_[:,0:dslr_.shape[1] -1]\n",
    "\n",
    "web_l=web_[:,-1]\n",
    "web=web_[:,0:web_.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "n_class = len(np.unique(ama_l))\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ama=ama.astype(np.float32)\n",
    "ama_l=ama_l.astype(np.float32)\n",
    "\n",
    "dslr=dslr.astype(np.float32)\n",
    "dslr_l=dslr_l.astype(np.float32)\n",
    "\n",
    "web=web.astype(np.float32)\n",
    "web_l=web_l.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ama_ll=ama_l.reshape(ama_l.shape[0], 1)\n",
    "\n",
    "dslr_ll=dslr_l.reshape(dslr_l.shape[0], 1)\n",
    "\n",
    "web_ll=web_l.reshape(web_l.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       ...,\n",
       "       [ 7.],\n",
       "       [ 5.],\n",
       "       [ 3.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ama_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "amaS, amaL=aTolistC(ama, ama_ll, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dslrS, dslrL=aTolistC(dslr, dslr_ll, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "webS, webL=aTolistC(web, web_ll, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_l=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "s2_l=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s11=  dslrS\n",
    "s11_l=  dslrL\n",
    "\n",
    "s22=  webS\n",
    "s22_l= webL\n",
    "\n",
    "T1= amaS\n",
    "T1_l= amaL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (source1)\n",
    "sou1= splitarr(s11, 21, s1_l)\n",
    "sou1_l=splitarr(s11_l, 21, s1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sou1_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (source2)\n",
    "sou2= splitarr(s22, 21, s2_l)\n",
    "sou2_l=splitarr(s22_l, 21, s2_l)\n",
    "#print(sou2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou2_l = sou2_l.ravel()\n",
    "sou1_l = sou1_l.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sou1_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh=np.unique(sou1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = ama\n",
    "tar_ll = ama_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_llk = np.copy(tar_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       ...,\n",
       "       [ 7.],\n",
       "       [ 5.],\n",
       "       [ 3.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Label update\n",
    "r=tar_ll.shape[0]\n",
    "for i in range (r):\n",
    "    \n",
    "    if tar_ll[i] >= 21:\n",
    "        \n",
    "        tar_llk[i] = 21\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(tar_llk))\n",
    "print(n_classes)\n",
    "tar_llk = tar_llk.ravel()\n",
    "#web_llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_l = tar_llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou1_l_h=tf.one_hot(sou1_l, n_classes)\n",
    "sou2_l_h=tf.one_hot(sou2_l, n_classes)\n",
    "tar_l_h=tf.one_hot(tar_l, n_classes)\n",
    "with tf.Session() as sess:\n",
    "    sou1_l_h=sess.run(sou1_l_h)\n",
    "    sou2_l_h=sess.run(sou2_l_h)\n",
    "    tar_l_h=sess.run(tar_l_h)\n",
    "#print(tar_l_h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b,c,d,e,g = BatchGen(10, sou1,sou1_l_h, sou2, sou2_l_h, web, tar_l_h )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \n",
    "    \n",
    "    initial = tf.random_normal(shape, stddev=0.1)\n",
    "    #initial = tf.truncated_normal(shape)#, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bise_variable(shape):\n",
    "   \n",
    "    initial = tf.random_normal(shape)\n",
    "    #initial = tf.constant(0.1, \"float32\", shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=sou1.shape[1]\n",
    "\n",
    "X1= tf.placeholder(tf.float32, [None, n_features], name='X1' ) # Source1 Input data \n",
    "Y1_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y1_ind')  # Source1 label index \n",
    "\n",
    "X2= tf.placeholder(tf.float32, [None, n_features], name='X2' ) # Source2 Input data \n",
    "Y2_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y2_ind')  # Source2 label index \n",
    "\n",
    "X3= tf.placeholder(tf.float32, [None, n_features], name='X3' ) # Target Input data \n",
    "Y3_ind = tf.placeholder(tf.float32, [None, n_classes], name= 'Y2_ind')  # Source3 label index \n",
    "l = tf.placeholder(tf.float32, [], name= 'l')  # gradient reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=n_features    \n",
    "num_hid1=1200\n",
    "num_hid2=600\n",
    "num_hid3=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_1=tf.Variable(tf.random_normal([num_inputs, num_hid1], stddev = 0.01), name= 'w1_1')\n",
    "b1_1=tf.Variable(tf.random_normal([num_hid1]), name = 'b1_1')\n",
    "\n",
    "w1_2=tf.Variable(tf.random_normal([num_inputs, num_hid1], stddev = 0.01), name= 'w1_2')\n",
    "b1_2=tf.Variable(tf.random_normal([num_hid1]), name = 'b1_2')\n",
    "\n",
    "w2_1=tf.Variable(tf.random_normal([num_hid1, num_hid2], stddev = 0.01), name= 'w2_1')\n",
    "b2_1=tf.Variable(tf.random_normal([num_hid2]), name = 'b2_1')\n",
    "\n",
    "w2_2=tf.Variable(tf.random_normal([num_hid1, num_hid2]), name= 'w2_2')\n",
    "b2_2=tf.Variable(tf.random_normal([num_hid2]), name = 'b2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5_1=tf.Variable(tf.random_normal([num_hid2, num_hid3], stddev = 0.01), name= 'w5_1')\n",
    "b5_1=tf.Variable(tf.random_normal([num_hid3]), name = 'b5_1')\n",
    "\n",
    "w6_1=tf.Variable(tf.random_normal([num_hid3, n_classes], stddev = 0.01), name= 'w6_1')\n",
    "b6_1=tf.Variable(tf.random_normal([n_classes]), name = 'b6_1')\n",
    "\n",
    "\n",
    "\n",
    "w5_2=tf.Variable(tf.random_normal([num_hid2, num_hid3], stddev = 0.01), name= 'w5_2')\n",
    "b5_2=tf.Variable(tf.random_normal([num_hid3]), name = 'b5_2')\n",
    "\n",
    "w6_2=tf.Variable(tf.random_normal([num_hid3, n_classes], stddev = 0.01), name= 'w6_2')\n",
    "b6_2=tf.Variable(tf.random_normal([n_classes]), name = 'b6_2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z1, z2, z3):\n",
    "    \n",
    "    #with tf.variable_scope(\"FeatureGenerator\",reuse=True):\n",
    "        \n",
    "    sh1_1=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(z1, w1_1) + b1_1))   # source1\n",
    "    #sh1_1 = Dropout(0.3)(sh1_1, training=True)\n",
    "    sh2_1=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(z2, w1_1) + b1_1))   # source2\n",
    "    #sh2_1 = Dropout(0.3)(sh2_1, training=True)\n",
    "    T1=tf.nn.relu(tf.matmul(z3, w1_1) + b1_1)    # Target\n",
    "        \n",
    "    sh1_2=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(sh1_1, w2_1) + b2_1))   # source1\n",
    "    #sh1_2 = Dropout(0.3)(sh1_2, training=True)\n",
    "    sh2_2=tf.layers.batch_normalization(tf.nn.relu(tf.matmul(sh2_1, w2_1) + b2_1))   # source2\n",
    "    #sh2_2 = Dropout(0.3)(sh2_2, training=True)\n",
    "    T2=tf.nn.relu(tf.matmul(T1, w2_1) + b2_1)    # Target\n",
    "        \n",
    "    return sh1_2, sh2_2, T2\n",
    "\n",
    "    \n",
    "    \n",
    "def classifier(x1, x2, t):\n",
    "    \n",
    "    #with tf.variable_scope(\"ClassClassifier\", reuse = True ):\n",
    "        \n",
    "    X11 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(x1, w5_1) + b5_1))   # 1st layer for source1\n",
    "    t11 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(t, w5_1) + b5_1))   # 1st layer target through source1\n",
    "    \n",
    "    p_logit_X1 = tf.matmul(X11, w6_1) + b6_1   # logit for source1\n",
    "    p_logit_t1 = tf.matmul(t11, w6_1) + b6_1   # logit for target through source1\n",
    "       \n",
    "        \n",
    "    X22 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(x2, w5_2) + b5_2))     # 2nd layer for source2\n",
    "    t22 = tf.layers.batch_normalization(tf.nn.relu(tf.matmul(t, w5_2) + b5_2))   # 2nd layer target through source2\n",
    "    \n",
    "    p_logit_X2 = tf.matmul(X22, w6_2) + b6_2   # logit for source1\n",
    "    p_logit_t2 = tf.matmul(t22, w6_2) + b6_2   # logit for target through source1\n",
    "        \n",
    "    p_logit_t = (p_logit_t1 + p_logit_t2)/2\n",
    "        \n",
    "    p_X1 = tf.nn.softmax(p_logit_X1)\n",
    "        \n",
    "    p_X2 = tf.nn.softmax(p_logit_X2)\n",
    "        \n",
    "    p_t = tf.nn.softmax(p_logit_t)\n",
    "    \n",
    "    p_t1 = tf.argmax(p_t,1)\n",
    "        \n",
    "        \n",
    "    return p_logit_X1, p_logit_X2, p_logit_t, p_X1, p_X2, p_t, p_t1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_f, s2_f, t_f = generator(X1, X2, X3)\n",
    "\n",
    "logit_x1, logit_x2, logit_t, px1, px2, p_t, p_t1 = classifier(s1_f, s2_f, t_f)\n",
    "\n",
    "t = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-7c7f9af4a3b1>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_loss_X1 = tf.nn.softmax_cross_entropy_with_logits(logits = logit_x1, labels = Y1_ind)\n",
    "p_loss_X2 = tf.nn.softmax_cross_entropy_with_logits(logits = logit_x2, labels = Y2_ind)\n",
    "\n",
    "p_loss_s1_s2 = tf.reduce_mean(tf.pow(s1_f - s2_f ,2), 1)\n",
    "\n",
    "\n",
    "\n",
    "loss_cl = tf.reduce_mean(p_loss_X1) + tf.reduce_mean(p_loss_X2)\n",
    "\n",
    "loss1 = loss_cl + p_loss_s1_s2\n",
    "\n",
    "p_kone=tf.gather(p_t,indices=[n_classes-1],axis=1)\n",
    "Ladv=-0.5*tf.reduce_mean(tf.log(p_kone+1e-8))-0.5*tf.reduce_mean((tf.log(1.0-(p_kone)+1e-8)))\n",
    "\n",
    "jh= p_t[:, :n_classes-1]\n",
    "\n",
    "kh=tf.argmax(jh, axis = 1)\n",
    "\n",
    "pu_l_h=tf.one_hot(kh, n_classes)\n",
    "\n",
    "\n",
    "gbp=p_t[:, -1]\n",
    "hg = tf.reduce_mean(jh, 1)\n",
    "alpha = 0.2\n",
    "hing=tf.abs(p_t[:, -1] - hg) - alpha\n",
    "\n",
    "row = jh.shape[0]\n",
    "gg=0.0\n",
    "\n",
    "bb=[]\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    hgbw = tf.cond(hing[i]<gg, lambda: hing[i], lambda:gg)\n",
    "    \n",
    "    bb.append(hgbw)\n",
    "\n",
    "hinge_loss=tf.reduce_mean(tf.stack(bb)) # hingh loss\n",
    "\n",
    "reg1 = tf.norm(w5_1,2) + tf.norm(w5_2,2) + tf.norm(w6_1,2) + tf.norm(w6_2,2)\n",
    "reg2 = tf.norm(w1_1,2) + tf.norm(w2_1,2)\n",
    "\n",
    "\n",
    "loss2 = loss_cl + Ladv  # classifier\n",
    "\n",
    "loss3 = loss1 - Ladv - hinge_loss  # Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_v = [w1_1, w2_1, b1_1, b2_1]\n",
    "C_v = [w5_1, w5_2, b5_1, b5_2, w6_1, w6_2, b6_1, b6_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_cl_op = tf.train.AdamOptimizer(0.001).minimize(loss1)\n",
    "\n",
    "loss1C_op = tf.train.AdamOptimizer(0.001).minimize(loss2, var_list = C_v)\n",
    "\n",
    "loss2G_op = tf.train.AdamOptimizer(0.001).minimize(loss3, var_list = G_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y1_ind, 1), tf.argmax(px1, 1)), tf.float32))\n",
    "\n",
    "p2_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y2_ind, 1), tf.argmax(px2, 1)), tf.float32))\n",
    "\n",
    "pt_acc =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y3_ind, 1), tf.argmax(p_t, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2939dc1f86be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtotal_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mX1_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX3_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatchGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou1_l_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou2_l_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtotal_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX3_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatchGen_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou1_l_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msou2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msou2_l_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsudo_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2aab89ee8997>\u001b[0m in \u001b[0;36mBatchGen\u001b[0;34m(batch_size, source1, label1, source2, label2, source3)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mla_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mla_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mla_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mla_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/biplab/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/biplab/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/biplab/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/biplab/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/biplab/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1307\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/home/biplab/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch=100\n",
    "batch_size=32\n",
    "dp1 = 0.0\n",
    "dp2 = 0.0\n",
    "dp11 = 0.0\n",
    "dp12 = 0.0\n",
    "dp13 = 0.0\n",
    "\n",
    "l1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "l2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "#ps_t = []\n",
    "#ps_t_l = []\n",
    "\n",
    "flag = 0\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        ps_t = []\n",
    "        ps_t_l = []\n",
    "        \n",
    "        if flag ==0:\n",
    "            total_batches = 50\n",
    "            \n",
    "            X1_batch,y1_batch, X2_batch,y2_batch, X3_batch=BatchGen(batch_size, sou1, sou1_l_h, sou2, sou2_l_h, tar)\n",
    "        else:\n",
    "            total_batches, X1_batch,y1_batch, X2_batch,y2_batch, X3_batch=BatchGen_p(batch_size, sou1, sou1_l_h, sou2,sou2_l_h, tar, psudo, psudo_l)\n",
    "        \n",
    "        flag = 0\n",
    "\n",
    "        for batch in range(total_batches):\n",
    "            \n",
    "            xx_1=X1_batch[batch]\n",
    "            #print(xx_1.shape)\n",
    "            \n",
    "            xx_2=X2_batch[batch]\n",
    "            #print(xx_2.shape)\n",
    "            \n",
    "            xx_3=X3_batch[batch]\n",
    "            #print(xx_3.shape)\n",
    "            \n",
    "            yy_1=y1_batch[batch]\n",
    "            yy_2=y2_batch[batch]\n",
    "            \n",
    "\n",
    "            \n",
    "            #_,batch_loss_cl  = sess.run([loss_cl_op, loss1],\n",
    "                                        #feed_dict={X1: xx_1, X2:xx_2, Y1_ind:yy_1, Y2_ind:yy_2, X3:xx_3})\n",
    "            \n",
    "            _,batch_loss1C, pu_one_hot, pu_logit, pu_argmax  = sess.run([loss1C_op, loss2, pu_l_h, jh, kh],\n",
    "                                        feed_dict={X1: xx_1, X2:xx_2, Y1_ind:yy_1, Y2_ind:yy_2, X3:xx_3})\n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "            _,batch_loss2G  = sess.run([loss2G_op, loss3],feed_dict={X1: xx_1, X2:xx_2, Y1_ind:yy_1, Y2_ind:yy_2, X3:xx_3})\n",
    "            \n",
    "            rr=0\n",
    "            for v in range(pu_one_hot.shape[0]):\n",
    "                \n",
    "                ff = pu_one_hot[v]\n",
    "                aa=pu_argmax[v]\n",
    "                dwq = tf.cast(ff, tf.int32)\n",
    "               \n",
    "                if pu_logit[rr, aa] >= 0.95:\n",
    "                    flag = 1\n",
    "                        \n",
    "                    ps_t.append(xx_3[v, :])\n",
    "                    ps_t_l.append(ff)\n",
    "                rr=rr+1\n",
    "\n",
    "        #ps_t.append(xx_1)\n",
    "        #ps_t_l.append(yy_1)\n",
    "        psudo=np.asarray(ps_t)\n",
    "        psudo_l = np.asarray(ps_t_l)\n",
    "        \n",
    "        \n",
    "        print(len(ps_t))\n",
    "        p = epoch%1\n",
    "        \n",
    "        if p==0:\n",
    "            print(\"epoch#####555555555555##########################\", epoch)\n",
    "                    \n",
    "            pas11 = sess.run([p1_acc], feed_dict={X1:tar, Y1_ind:tar_l_h})\n",
    "            pas12 = sess.run([p2_acc], feed_dict={X2:tar, Y2_ind:tar_l_h})\n",
    "            pass_avg = sess.run([pt_acc], feed_dict={X3:tar, Y3_ind:tar_l_h})\n",
    "            lab = sess.run([p_t1], feed_dict={X3:tar, Y3_ind:tar_l_h})\n",
    "            \n",
    "            \n",
    "            lab = np.matrix.transpose(np.asarray(lab))\n",
    "\n",
    "            OS = recall_score(tar_l, lab, labels=l2, average='macro')\n",
    "            OS_star = recall_score(tar_l, lab, labels=l1, average='macro')\n",
    "\n",
    "       \n",
    "            print('target from source1', pas11)\n",
    "            print('target from source2', pas12)\n",
    "            print('OS', OS)\n",
    "            print('OS-star', OS_star)\n",
    "            print('target average', pass_avg)\n",
    "        \n",
    "        if OS >= dp11:\n",
    "            \n",
    "            dp11=OS\n",
    "            dp12=OS_star\n",
    "            dp13 = pass_avg[0]\n",
    "            dp1 = pas11[0]\n",
    "            dp2 = pas12[0]\n",
    "            ep11=epoch\n",
    "\n",
    "      \n",
    "        print('print highest source1: %f, epoch%f'%(dp1, ep11))\n",
    "        print('print highest source2: %f, epoch%f'%(dp2, ep11))\n",
    "        print('print highest OS: %f, epoch%f'%(dp11, ep11))\n",
    "        print('print highest OS-star: %f, epoch%f'%(dp12, ep11))\n",
    "        print('print highest ALL: %f, epoch%f'%(dp13, ep11))\n",
    "                            \n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "    sess.close()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
